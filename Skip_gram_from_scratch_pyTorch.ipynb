{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Skip_gram_for_SNA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKhbTJQ85RYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import urllib\n",
        "from io import open\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqs3TZMC5TOg",
        "colab_type": "code",
        "outputId": "83d83afc-2191-4425-a7ba-56e27d3f2de8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget http://mattmahoney.net/dc/text8.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-24 13:57:50--  http://mattmahoney.net/dc/text8.zip\n",
            "Resolving mattmahoney.net (mattmahoney.net)... 67.195.197.75\n",
            "Connecting to mattmahoney.net (mattmahoney.net)|67.195.197.75|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 31344016 (30M) [application/zip]\n",
            "Saving to: ‘text8.zip’\n",
            "\n",
            "text8.zip           100%[===================>]  29.89M   710KB/s    in 44s     \n",
            "\n",
            "2019-12-24 13:58:34 (691 KB/s) - ‘text8.zip’ saved [31344016/31344016]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ts3qDX06j8k",
        "colab_type": "code",
        "outputId": "77a32920-3099-4817-f0fd-8e7a7e02b5a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!unzip text8.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  text8.zip\n",
            "  inflating: text8                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uM8gKClEvb0Z",
        "colab_type": "code",
        "outputId": "67c0aee8-2d97-43c4-b649-cd8737b07f5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NogdNqB1NmcO",
        "colab_type": "text"
      },
      "source": [
        "Читаем данные"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqGWT2m98yhI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def read_own_data(filename):\n",
        "    \"\"\"\n",
        "    read your own data.\n",
        "    :param filename:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    print('reading data...')\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        data = f.read().split()\n",
        "    print('corpus size', len(data))\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OTYSh_Utwbc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_dataset(words, n_words):\n",
        "    \"\"\"\n",
        "    build dataset\n",
        "    :param words: corpus\n",
        "    :param n_words: learn most common n_words\n",
        "    :return:\n",
        "        - data: [word_index]\n",
        "        - count: [ [word_index, word_count], ]\n",
        "        - dictionary: {word_str: word_index}\n",
        "        - reversed_dictionary: {word_index: word_str}\n",
        "    \"\"\"\n",
        "    count = [['UNK', -1]]\n",
        "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
        "    dictionary = dict()\n",
        "    for word, _ in count:\n",
        "        dictionary[word] = len(dictionary)\n",
        "    data = list()\n",
        "    unk_count = 0\n",
        "    for word in words:\n",
        "        if word in dictionary:\n",
        "            index = dictionary[word]\n",
        "        else:\n",
        "            index = 0  # UNK index is 0\n",
        "            unk_count += 1\n",
        "        data.append(index)\n",
        "    count[0][1] = unk_count\n",
        "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
        "    return data, count, dictionary, reversed_dictionary\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWbV0IEeCD31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def noise(vocabs, word_count):\n",
        "    \"\"\"\n",
        "    generate noise distribution\n",
        "    :param vocabs:\n",
        "    :param word_count:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    Z = 0.001\n",
        "    unigram_table = []\n",
        "    num_total_words = sum([c for w, c in word_count])\n",
        "    for vo in vocabs:\n",
        "        unigram_table.extend([vo] * int(((word_count[vo][1]/num_total_words)**0.75)/Z))\n",
        "\n",
        "    print(\"vocabulary size\", len(vocabs))\n",
        "    print(\"unigram_table size:\", len(unigram_table))\n",
        "    return unigram_table"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgXkoG7YvFye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataPipeline:\n",
        "    def __init__(self, data, vocabs, word_count, data_index=0, use_noise_neg=True):\n",
        "        self.data = data\n",
        "        self.data_index = data_index\n",
        "        if use_noise_neg:\n",
        "            self.unigram_table = noise(vocabs, word_count)\n",
        "        else:\n",
        "            self.unigram_table = vocabs\n",
        "\n",
        "    def get_neg_data(self, batch_size, num, target_inputs):\n",
        "        \"\"\"\n",
        "        sample the negative data. Don't use np.random.choice(), it is very slow.\n",
        "        :param batch_size: int\n",
        "        :param num: int\n",
        "        :param target_inputs: []\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        neg = np.zeros((num))\n",
        "        for i in range(batch_size):\n",
        "            delta = random.sample(self.unigram_table, num)\n",
        "            while target_inputs[i] in delta:\n",
        "                delta = random.sample(self.unigram_table, num)\n",
        "            neg = np.vstack([neg, delta])\n",
        "        return neg[1: batch_size + 1]\n",
        "\n",
        "    def generate_batch(self, batch_size, num_skips, skip_window):\n",
        "        \"\"\"\n",
        "        get the data batch\n",
        "        :param batch_size:\n",
        "        :param num_skips:\n",
        "        :param skip_window:\n",
        "        :return: target batch and context batch\n",
        "        \"\"\"\n",
        "        assert batch_size % num_skips == 0\n",
        "        assert num_skips <= 2 * skip_window\n",
        "        batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
        "        labels = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
        "        span = 2 * skip_window + 1  # [ skip_window, target, skip_window ]\n",
        "        buffer = collections.deque(maxlen=span)\n",
        "        for _ in range(span):\n",
        "            buffer.append(self.data[self.data_index])\n",
        "            self.data_index = (self.data_index + 1) % len(self.data)\n",
        "        for i in range(batch_size // num_skips):\n",
        "            target = skip_window\n",
        "            targets_to_avoid = [skip_window]\n",
        "            for j in range(num_skips):\n",
        "                while target in targets_to_avoid:\n",
        "                    target = random.randint(0, span - 1)\n",
        "                targets_to_avoid.append(target)\n",
        "                batch[i * num_skips + j] = buffer[skip_window]\n",
        "                labels[i * num_skips + j] = buffer[target]\n",
        "            buffer.append(self.data[self.data_index])\n",
        "            self.data_index = (self.data_index + 1) % len(self.data)\n",
        "        self.data_index = (self.data_index + len(self.data) - span) % len(self.data)\n",
        "        return batch, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00FZ1CXwLqxW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class SkipGramNeg(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim):\n",
        "        super(SkipGramNeg, self).__init__()\n",
        "        self.input_emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.output_emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.log_sigmoid = nn.LogSigmoid()\n",
        "\n",
        "        initrange = (2.0 / (vocab_size + emb_dim)) ** 0.5  # Xavier init\n",
        "        self.input_emb.weight.data.uniform_(-initrange, initrange)\n",
        "        self.output_emb.weight.data.uniform_(-0, 0)\n",
        "\n",
        "\n",
        "    def forward(self, target_input, context, neg):\n",
        "        \"\"\"\n",
        "        :param target_input: [batch_size]\n",
        "        :param context: [batch_size]\n",
        "        :param neg: [batch_size, neg_size]\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # u,v: [batch_size, emb_dim]\n",
        "        v = self.input_emb(target_input)\n",
        "        u = self.output_emb(context)\n",
        "        # positive_val: [batch_size]\n",
        "        positive_val = self.log_sigmoid(torch.sum(u * v, dim=1)).squeeze()\n",
        "\n",
        "        # u_hat: [batch_size, neg_size, emb_dim]\n",
        "        u_hat = self.output_emb(neg)\n",
        "        # [batch_size, neg_size, emb_dim] x [batch_size, emb_dim, 1] = [batch_size, neg_size, 1]\n",
        "        # neg_vals: [batch_size, neg_size]\n",
        "        neg_vals = torch.bmm(u_hat, v.unsqueeze(2)).squeeze(2)\n",
        "        # neg_val: [batch_size]\n",
        "        neg_val = self.log_sigmoid(-torch.sum(neg_vals, dim=1)).squeeze()\n",
        "\n",
        "        loss = positive_val + neg_val\n",
        "        return -loss.mean()\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        return self.input_emb(inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qujtGHX0v8lH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Oil0XQUMRkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "from torch.optim import SGD\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "class Word2Vec:\n",
        "    def __init__(self, data_path, vocabulary_size, embedding_size, learning_rate=1.0):\n",
        "\n",
        "        self.corpus = read_own_data(data_path)\n",
        "\n",
        "        self.data, self.word_count, self.word2index, self.index2word = build_dataset(self.corpus,\n",
        "                                                                    vocabulary_size)\n",
        "        self.vocabs = list(set(self.data))\n",
        "\n",
        "        self.model: SkipGramNeg = SkipGramNeg(vocabulary_size, embedding_size).to(device)\n",
        "        self.model_optim = SGD(self.model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "    def train(self, train_steps=4000, skip_window=2, num_skips=2, num_neg=7, batch_size=256, data_offest=0, vali_size=3, output_dir='out'):\n",
        "        self.outputdir = os.mkdir(output_dir)\n",
        "\n",
        "        avg_loss = 0\n",
        "        pipeline = DataPipeline(self.data, self.vocabs ,self.word_count, data_offest)\n",
        "        vali_examples = random.sample(self.vocabs, vali_size)\n",
        "        losses = []\n",
        "        progress_bar = tqdm(total=train_steps, desc='How many steps were done')\n",
        "\n",
        "        for step in range(train_steps):\n",
        "            batch_inputs, batch_labels = pipeline.generate_batch(batch_size, num_skips, skip_window)\n",
        "            batch_neg = pipeline.get_neg_data(batch_size, num_neg, batch_inputs)\n",
        "\n",
        "            batch_inputs = torch.tensor(batch_inputs, dtype=torch.long).to(device)\n",
        "            batch_labels = torch.tensor(batch_labels, dtype=torch.long).to(device)\n",
        "            batch_neg = torch.tensor(batch_neg, dtype=torch.long).to(device)\n",
        "\n",
        "            loss = self.model(batch_inputs, batch_labels, batch_neg)\n",
        "            self.model_optim.zero_grad()\n",
        "            loss.backward()\n",
        "            self.model_optim.step()\n",
        "\n",
        "            avg_loss += loss.item()\n",
        "            losses.append(loss.item())\n",
        "            progress_bar.set_postfix(train_loss = np.mean(losses[-500:]))\n",
        "\n",
        "            progress_bar.update(1)\n",
        "\n",
        "        # save model at last\n",
        "        torch.save(self.model.state_dict(), self.outputdir + '/model_step%d.pt' % train_steps)\n",
        "\n",
        "    def save_model(self, out_path):\n",
        "        torch.save(self.model.state_dict(), out_path + '/model.pt')\n",
        "\n",
        "    def get_list_vector(self):\n",
        "        sd = self.model.state_dict()\n",
        "        return sd['input_emb.weight'].tolist()\n",
        "\n",
        "    def load_model(self, model_path):\n",
        "        self.model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    def vector(self, index):\n",
        "        self.model.predict(index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXlZqJKFMU5u",
        "colab_type": "code",
        "outputId": "4b2b1136-dfd5-4a7b-e903-e21396923afe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "source": [
        "word2vec = Word2Vec(data_path='text8',\n",
        "                    vocabulary_size=200000,\n",
        "                    embedding_size=300)\n",
        "word2vec.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading data...\n",
            "corpus size 17005207\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rHow many steps were done:   0%|          | 0/4000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "vocabulary size 200000\n",
            "unigram_table size: 2821\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "How many steps were done: 100%|██████████| 4000/4000 [31:17<00:00,  2.13it/s, train_loss=0.937]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-6fd1df30409c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                     \u001b[0mvocabulary_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     embedding_size=300)\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-1ca1129ce6c1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_steps, skip_window, num_skips, num_neg, batch_size, data_offest, vali_size, output_dir)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# save model at last\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputdir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/model_step%d.pt'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtrain_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'str'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eechLnW0kWG5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for instance in list(tqdm._instances):\n",
        "    tqdm._decr_instances(instance)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZitJUxixgl0s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rmdir out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCTwKw1BQCFD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectors = word2vec.get_list_vector()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8e8h0UT3by9",
        "colab_type": "code",
        "outputId": "cbecf132-df9c-45ca-943b-825cd28024f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/tmikolov/word2vec/master/questions-words.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-24 14:42:06--  https://raw.githubusercontent.com/tmikolov/word2vec/master/questions-words.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 603955 (590K) [text/plain]\n",
            "Saving to: ‘questions-words.txt’\n",
            "\n",
            "\rquestions-words.txt   0%[                    ]       0  --.-KB/s               \rquestions-words.txt 100%[===================>] 589.80K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2019-12-24 14:42:06 (27.8 MB/s) - ‘questions-words.txt’ saved [603955/603955]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRW0AfoiNeJ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "# Training times in seconds\n",
        "\n",
        "def print_accuracy(model, questions_file):\n",
        "    print('Evaluating...\\n')\n",
        "    acc = model.accuracy(questions_file)\n",
        "\n",
        "    sem_correct = sum((len(acc[i]['correct']) for i in range(5)))\n",
        "    sem_total = sum((len(acc[i]['correct']) + len(acc[i]['incorrect'])) for i in range(5))\n",
        "    sem_acc = 100*float(sem_correct)/sem_total\n",
        "    print('\\nSemantic: {:d}/{:d}, Accuracy: {:.2f}%'.format(sem_correct, sem_total, sem_acc))\n",
        "    \n",
        "    syn_correct = sum((len(acc[i]['correct']) for i in range(5, len(acc)-1)))\n",
        "    syn_total = sum((len(acc[i]['correct']) + len(acc[i]['incorrect'])) for i in range(5,len(acc)-1))\n",
        "    syn_acc = 100*float(syn_correct)/syn_total\n",
        "    print('Syntactic: {:d}/{:d}, Accuracy: {:.2f}%\\n'.format(syn_correct, syn_total, syn_acc))\n",
        "    return (sem_acc, syn_acc)\n",
        "\n",
        "word_analogies_file = 'questions-words.txt'\n",
        "accuracies = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EwQOs6mNnEb",
        "colab_type": "code",
        "outputId": "fa61f233-f4ef-4386-9130-0e14cbac3ffc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "print('Accuracy for Word2Vec:')\n",
        "accuracies.append(print_accuracy(model, word_analogies_file))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `accuracy` (Method will be removed in 4.0.0, use self.evaluate_word_analogies() instead).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "2019-12-24 14:46:02,883 : INFO : precomputing L2-norms of word weight vectors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy for Word2Vec:\n",
            "Evaluating...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n",
            "2019-12-24 14:46:06,319 : INFO : capital-common-countries: 0.0% (0/506)\n",
            "2019-12-24 14:46:15,286 : INFO : capital-world: 0.0% (0/1452)\n",
            "2019-12-24 14:46:16,959 : INFO : currency: 0.0% (0/268)\n",
            "2019-12-24 14:46:26,837 : INFO : city-in-state: 0.0% (0/1571)\n",
            "2019-12-24 14:46:28,785 : INFO : family: 0.0% (0/306)\n",
            "2019-12-24 14:46:33,522 : INFO : gram1-adjective-to-adverb: 0.0% (0/756)\n",
            "2019-12-24 14:46:35,476 : INFO : gram2-opposite: 0.0% (0/306)\n",
            "2019-12-24 14:46:43,370 : INFO : gram3-comparative: 0.0% (0/1260)\n",
            "2019-12-24 14:46:46,592 : INFO : gram4-superlative: 0.0% (0/506)\n",
            "2019-12-24 14:46:52,898 : INFO : gram5-present-participle: 0.0% (0/992)\n",
            "2019-12-24 14:47:01,525 : INFO : gram6-nationality-adjective: 0.1% (1/1371)\n",
            "2019-12-24 14:47:09,911 : INFO : gram7-past-tense: 0.0% (0/1332)\n",
            "2019-12-24 14:47:16,146 : INFO : gram8-plural: 0.0% (0/992)\n",
            "2019-12-24 14:47:20,201 : INFO : gram9-plural-verbs: 0.0% (0/650)\n",
            "2019-12-24 14:47:20,206 : INFO : total: 0.0% (1/12268)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Semantic: 0/4103, Accuracy: 0.00%\n",
            "Syntactic: 1/8165, Accuracy: 0.01%\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2ROJPx5RCPA",
        "colab_type": "code",
        "outputId": "019fb17f-c572-4477-c765-b3fb9e99455b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import pandas as pd\n",
        "semeval = pd.read_csv('semeval.csv', sep=',')\n",
        "semeval.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>type</th>\n",
              "      <th>word1</th>\n",
              "      <th>word2</th>\n",
              "      <th>word3</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2c</td>\n",
              "      <td>water</td>\n",
              "      <td>drop</td>\n",
              "      <td>hour</td>\n",
              "      <td>seconds</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2c</td>\n",
              "      <td>mile</td>\n",
              "      <td>yard</td>\n",
              "      <td>hour</td>\n",
              "      <td>seconds</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2c</td>\n",
              "      <td>time</td>\n",
              "      <td>moment</td>\n",
              "      <td>hour</td>\n",
              "      <td>seconds</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2c</td>\n",
              "      <td>water</td>\n",
              "      <td>drop</td>\n",
              "      <td>feet</td>\n",
              "      <td>inches</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2c</td>\n",
              "      <td>mile</td>\n",
              "      <td>yard</td>\n",
              "      <td>feet</td>\n",
              "      <td>inches</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0 type  word1   word2 word3   target\n",
              "0           0   2c  water    drop  hour  seconds\n",
              "1           1   2c   mile    yard  hour  seconds\n",
              "2           2   2c   time  moment  hour  seconds\n",
              "3           3   2c  water    drop  feet   inches\n",
              "4           4   2c   mile    yard  feet   inches"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc7-RYKZd0tx",
        "colab_type": "code",
        "outputId": "cc261721-2544-41b7-ec12-45ed93c889ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "word2vec.index2word"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hirvCKL8Sh5N",
        "colab_type": "code",
        "outputId": "e9ba90b3-060b-4845-8800-eaba70cfefae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vec = open('vectors.txt', 'w')\n",
        "vec.write(str(len(vectors))+' 300\\n')\n",
        "for indx, vector in enumerate(vectors):\n",
        "  line = word2vec.index2word[indx]+' '\n",
        "  for i in vector:\n",
        "    line+=str(i)+' '\n",
        "  if indx == (len(vectors)-1):\n",
        "    print('last')\n",
        "    vec.write(line[:-2])\n",
        "  else:\n",
        "    vec.write(line[:-2]+'\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "last\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jWnkw6jRRCP",
        "colab_type": "code",
        "outputId": "5f615377-6678-4dfc-de06-b727a1f9d650",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import gensim\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('vectors.txt', binary=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B410JtxyWTFW",
        "colab_type": "code",
        "outputId": "e4cbbe18-a37e-4051-ca8e-a9fad93120e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "model.most_similar(positive=['king', 'woman'], negative=['man'], topn=15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('for', 0.9849818348884583),\n",
              " ('on', 0.9849700927734375),\n",
              " ('that', 0.9849443435668945),\n",
              " ('to', 0.9848905801773071),\n",
              " ('is', 0.9848145246505737),\n",
              " ('and', 0.9847832322120667),\n",
              " ('by', 0.9847445487976074),\n",
              " ('of', 0.9847289323806763),\n",
              " ('wa', 0.9846924543380737),\n",
              " ('with', 0.9846853613853455),\n",
              " ('country', 0.9846317768096924),\n",
              " ('ha', 0.9846177101135254),\n",
              " ('which', 0.9845396876335144),\n",
              " ('from', 0.9845099449157715),\n",
              " ('in', 0.9844862222671509)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMrI4IiPaGuz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for instance in list(tqdm._instances):\n",
        "    tqdm._decr_instances(instance)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2sBtfBoXHb5",
        "colab_type": "code",
        "outputId": "3513cf88-fa0e-496e-cbbf-574c526ab16c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "s = 0\n",
        "j = 0\n",
        "for i in tqdm(range(semeval.shape[0])):\n",
        "  a, b, c = semeval['word1'][i], semeval['word3'][i], semeval['word2'][i]\n",
        "  try:\n",
        "    res = model.most_similar(positive=[a, b], negative=[ c ], topn=10)\n",
        "    words = [i for i,j in res]\n",
        "    if semeval['target'][i] in words:\n",
        "      s+=1\n",
        "  except KeyError:\n",
        "    j+=1\n",
        "    continue\n",
        "\n",
        "print(s/(semeval.shape[0]-j))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/10014 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n",
            "100%|██████████| 10014/10014 [05:56<00:00, 23.87it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.0007883770694898074\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yogEufVUXmnl",
        "colab_type": "code",
        "outputId": "20506216-e43e-4579-de35-a6a0f6cdbaa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "s"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-XPNGAxaufI",
        "colab_type": "code",
        "outputId": "cead85e1-d231-424f-f4c2-541446be42a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "j"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1185"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKxRPsW-dHIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}